{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgRicJtbDuBa"
   },
   "source": [
    "# seq2seq构建的聊天机器人应用\n",
    "我们来使用seq2seq框架完成一个聊天机器人构建的任务，我给大家准备了一些对话语料，我们使用这份数据来构建聊天机器人的AI应用。在此之前，我们先了解一下原有的翻译系统需要准备的语料格式，我们把中文数据处理成格式一致的形态。\n",
    "\n",
    "我们先拉取一份样例数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "D6WFcziSDuBb",
    "outputId": "3ea4f8d0-2a2a-4912-81b4-941ce80b49f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n",
      "Download training dataset train.en and train.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 12.9M  100 12.9M    0     0  4415k      0  0:00:03  0:00:03 --:--:-- 4415k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17.2M  100 17.2M    0     0  5386k      0  0:00:03  0:00:03 --:--:-- 5386k\n",
      "Download dev dataset tst2012.en and tst2012.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  136k  100  136k    0     0   188k      0 --:--:-- --:--:-- --:--:--  188k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  183k  100  183k    0     0   253k      0 --:--:-- --:--:-- --:--:--  253k\n",
      "Download test dataset tst2013.en and tst2013.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  129k  100  129k    0     0   196k      0 --:--:-- --:--:-- --:--:--  196k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  179k  100  179k    0     0   246k      0 --:--:-- --:--:-- --:--:--  246k\n",
      "Download vocab file vocab.en and vocab.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  136k  100  136k    0     0   207k      0 --:--:-- --:--:-- --:--:--  208k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 46767  100 46767    0     0   102k      0 --:--:-- --:--:-- --:--:--  102k\n"
     ]
    }
   ],
   "source": [
    "%cd nmt\n",
    "!bash nmt/scripts/download_iwslt15.sh /tmp/nmt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Cp3DTKmDuBe"
   },
   "source": [
    "**查看一下包含的文件**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_KeeIZNaDuBg",
    "outputId": "055d8dfa-c074-476c-de5a-d7da129873e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.en  tst2012.en  tst2013.en  vocab.en\n",
      "train.vi  tst2012.vi  tst2013.vi  vocab.vi\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/nmt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CSnj8lPDuBj"
   },
   "source": [
    "**看一下源语言与目标语言的格式，以及对应的数据量**\n",
    "\n",
    "可以看到都是做过tokenization之后的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "GXhLcDo7DuBk",
    "outputId": "caef6cfe-054e-49f9-cd43-55521aa42b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Pike : The science behind a climate headline\n",
      "In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
      "I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
      "Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
      "They are both two branches of the same field of atmospheric science .\n",
      "Recently the headlines looked like this when the Intergovernmental Panel on Climate Change , or IPCC , put out their report on the state of understanding of the atmospheric system .\n",
      "That report was written by 620 scientists from 40 countries .\n",
      "They wrote almost a thousand pages on the topic .\n",
      "And all of those pages were reviewed by another 400-plus scientists and reviewers , from 113 countries .\n",
      "It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .\n"
     ]
    }
   ],
   "source": [
    "!head -10 /tmp/nmt_data/train.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-vne-jIUDuBm",
    "outputId": "cef0d6f5-bf20-49b4-b6e5-f6b61a26db63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133317 /tmp/nmt_data/train.en\n"
     ]
    }
   ],
   "source": [
    "!wc -l /tmp/nmt_data/train.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7ySPsk0DuBp"
   },
   "source": [
    "**还需要准备好vocabulary词表**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "P1-RpoqSDuBq",
    "outputId": "d7eaa38e-3a76-4277-b8fd-31960922a80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "Rachel\n",
      ":\n",
      "The\n",
      "science\n",
      "behind\n",
      "a\n",
      "climate\n"
     ]
    }
   ],
   "source": [
    "!head -10 /tmp/nmt_data/vocab.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W-_HmH1vDuBt",
    "outputId": "d1271a6b-05d4-4631-9531-769f98ad68e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17191 /tmp/nmt_data/vocab.en\n"
     ]
    }
   ],
   "source": [
    "!wc -l /tmp/nmt_data/vocab.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gbue9sZYDuBw",
    "outputId": "38e898d3-0d66-4a5d-ad49-f4d73cd5bd44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7709 /tmp/nmt_data/vocab.vi\n"
     ]
    }
   ],
   "source": [
    "!wc -l /tmp/nmt_data/vocab.vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7W7aj-iEr7k"
   },
   "source": [
    "### 聊天机器人语料\n",
    "这里列举一些从网络中找到的用于训练中文（英文）聊天机器人的对话语料。\n",
    "\n",
    "1. [dgk_shooter_min.conv.zip](https://github.com/rustch3n/dgk_lost_conv)\n",
    "<br>中文电影对白语料，噪音比较大，许多对白问答关系没有对应好\n",
    "\n",
    "2. [The NUS SMS Corpus](https://github.com/kite1988/nus-sms-corpus)\n",
    "<br>包含中文和英文短信息语料，据说是世界最大公开的短消息语料\n",
    "\n",
    "3. [ChatterBot中文基本聊天语料](https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data)\n",
    "<br>ChatterBot聊天引擎提供的一点基本中文聊天语料，量很少，但质量比较高\n",
    "\n",
    "4. [Datasets for Natural Language Processing](https://github.com/karthikncode/nlp-datasets)\n",
    "<br>这是他人收集的自然语言处理相关数据集，主要包含Question Answering，Dialogue Systems， Goal-Oriented Dialogue Systems三部分，都是英文文本。可以使用机器翻译为中文，供中文对话使用\n",
    "\n",
    "5. [小黄鸡](https://github.com/rustch3n/dgk_lost_conv/tree/master/results)\n",
    "<br>据传这就是小黄鸡的语料：xiaohuangji50w_fenciA.conv.zip （已分词） 和 xiaohuangji50w_nofenci.conv.zip （未分词）\n",
    "\n",
    "6. [白鹭时代中文问答语料](https://github.com/Samurais/egret-wenda-corpus)\n",
    "<br>由白鹭时代官方论坛问答板块10,000+ 问题中，选择被标注了“最佳答案”的纪录汇总而成。人工review raw data，给每一个问题，一个可以接受的答案。目前，语料库只包含2907个问答。([备份](./egret-wenda-corpus.zip))\n",
    "\n",
    "7. [Chat corpus repository](https://github.com/Marsan-Ma/chat_corpus)\n",
    "<br>chat corpus collection from various open sources\n",
    "<br>包括：开放字幕、英文电影字幕、中文歌词、英文推文\n",
    "\n",
    "8. [保险行业QA语料库](https://github.com/Samurais/insuranceqa-corpus-zh)\n",
    "<br>通过翻译 [insuranceQA](https://github.com/shuzi/insuranceQA)产生的数据集。train_data含有问题12,889条，数据 141779条，正例：负例 = 1:10； test_data含有问题2,000条，数据 22000条，正例：负例 = 1:10；valid_data含有问题2,000条，数据 22000条，正例：负例 = 1:10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxWshRykDuBz"
   },
   "source": [
    "**我们下载小黄鸡语料，并对它做一个处理，使得它符合seq2seq模型的输入格式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "aZiHEojSFE3h",
    "outputId": "28655b79-ed94-47b3-8d9b-7aa9d62c7ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-27 07:52:17--  https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip [following]\n",
      "--2019-01-27 07:52:17--  https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10232473 (9.8M) [application/zip]\n",
      "Saving to: ‘xiaohuangji50w_nofenci.conv.zip’\n",
      "\n",
      "\r",
      "          xiaohuang   0%[                    ]       0  --.-KB/s               \r",
      "         xiaohuangj  17%[==>                 ]   1.74M  8.70MB/s               \r",
      "        xiaohuangji  59%[==========>         ]   5.84M  14.5MB/s               \r",
      "xiaohuangji50w_nofe 100%[===================>]   9.76M  19.1MB/s    in 0.5s    \n",
      "\n",
      "2019-01-27 07:52:18 (19.1 MB/s) - ‘xiaohuangji50w_nofenci.conv.zip’ saved [10232473/10232473]\n",
      "\n",
      "Archive:  xiaohuangji50w_nofenci.conv.zip\n",
      "  inflating: xiaohuangji50w_nofenci.conv  \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip\n",
    "!unzip xiaohuangji50w_nofenci.conv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "pZBDdmhVFQrd",
    "outputId": "6ff94090-282a-4cee-9ded-56607ef9d76d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "Q 呵呵\n",
      "A 是王若猫的。\n",
      "E\n",
      "Q 不是\n",
      "A 那是什么？\n",
      "E\n",
      "Q 怎么了\n",
      "A 我很难过，安慰我~\n",
      "E\n",
      "Q 开心点哈,一切都会好起来\n",
      "A 嗯 会的\n",
      "E\n",
      "Q 我还喜欢她,怎么办\n",
      "A 我帮你告诉她？发短信还是打电话？\n",
      "E\n",
      "Q 短信\n",
      "A 嗯嗯。我也相信\n",
      "E\n",
      "Q 你知道谁么\n",
      "A 肯定不是我，是阮德培\n",
      "E\n",
      "Q 许兵是谁\n",
      "A 吴院四班小帅哥\n",
      "E\n",
      "Q 这么假\n",
      "A 三鹿奶粉也假，不一样的卖啊\n",
      "E\n",
      "Q 许兵是傻逼\n",
      "A 被你发现了。\n"
     ]
    }
   ],
   "source": [
    "!perl -pi.bak -e 's/(E\\s)/\\1Q /g' xiaohuangji50w_nofenci.conv\n",
    "!perl -pi.bak -e 's/(Q M)/Q/g' xiaohuangji50w_nofenci.conv\n",
    "!perl -pi.bak -e 's/(M )/A /g' xiaohuangji50w_nofenci.conv\n",
    "!head -30 xiaohuangji50w_nofenci.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NDF6ipgHN_n-"
   },
   "outputs": [],
   "source": [
    "text = open(\"xiaohuangji50w_nofenci.conv\").read().split(\"E\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5PMg5KV8OGOj",
    "outputId": "576f2e61-c9a7-4fae-a9b2-57c85f5c7717"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q 呵呵\\nA 是王若猫的。\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFgSeknoF0IO"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def split_conv(in_f, out_q, out_a):\n",
    "  out_question = open(out_q, 'w')\n",
    "  out_answer = open(out_a, 'w')\n",
    "  text = open(in_f).read().split(\"E\\n\")\n",
    "  for pair in text:\n",
    "    # 句子长度太短的问题对话，跳过\n",
    "    if len(pair)<=4:\n",
    "      continue\n",
    "    # 切分问题和回答\n",
    "    contents = pair.split(\"\\n\")\n",
    "    out_question.write(\" \".join(jieba.lcut(contents[0].strip(\"Q \")))+\"\\n\")\n",
    "    out_answer.write(\" \".join(jieba.lcut(contents[1].strip(\"A \")))+\"\\n\")\n",
    "  out_question.close()\n",
    "  out_answer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WMjrPRkGvFZ"
   },
   "outputs": [],
   "source": [
    "in_f = \"xiaohuangji50w_nofenci.conv\"\n",
    "out_q = 'question.file'\n",
    "out_a = 'answer.file'\n",
    "split_conv(in_f, out_q, out_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "RDFtDEZhJkmq",
    "outputId": "90544356-fbe8-48ac-ade2-224ade638dba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "呵呵\n",
      "不是\n",
      "怎么 了\n",
      "开心 点哈 , 一切 都 会 好 起来\n",
      "我 还 喜欢 她 , 怎么办\n",
      "短信\n",
      "你 知道 谁 么\n",
      "许兵 是 谁\n",
      "这么 假\n",
      "许兵 是 傻 逼\n"
     ]
    }
   ],
   "source": [
    "!head -10 question.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Gc2KjA7KKFr4",
    "outputId": "0f319e2c-519f-467f-a961-9572f76376ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是 王若 猫 的 。\n",
      "那 是 什么 ？\n",
      "我 很 难过 ， 安慰 我 ~\n",
      "嗯   会 的\n",
      "我 帮 你 告诉 她 ？ 发短信 还是 打电话 ？\n",
      "嗯 嗯 。 我 也 相信\n",
      "肯定 不是 我 ， 是 阮德培\n",
      "吴院 四班 小帅哥\n",
      "三鹿 奶粉 也 假 ， 不 一样 的 卖 啊\n",
      "被 你 发现 了 。\n"
     ]
    }
   ],
   "source": [
    "!head -10 answer.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eIejL95sHNdk",
    "outputId": "1efeed65-3fa7-47ed-b3b7-9675ac45b0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454131 question.file\n"
     ]
    }
   ],
   "source": [
    "!wc -l question.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Aa29ly54HQok",
    "outputId": "ace052ec-bbb5-4c32-f159-7651a38dcfe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454131 answer.file\n"
     ]
    }
   ],
   "source": [
    "!wc -l answer.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4XWRgjyDuB0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_vocab(in_f, out_f):\n",
    "    vocab_dic = {}\n",
    "    for line in open(in_f, encoding='utf-8'):\n",
    "        words = line.strip().split(\" \")\n",
    "        for word in words:\n",
    "            # 保留汉字内容\n",
    "            if not re.match(r\"[\\u4e00-\\u9fa5]+\", word):\n",
    "                continue\n",
    "            try:\n",
    "                vocab_dic[word] += 1\n",
    "            except:\n",
    "                vocab_dic[word] = 1\n",
    "    out = open(out_f, 'w', encoding='utf-8')\n",
    "    out.write(\"<unk>\\n<s>\\n</s>\\n\")\n",
    "    vocab = sorted(vocab_dic.items(),key = lambda x:x[1],reverse = True)\n",
    "    for word in [x[0] for x in vocab[:80000]]:\n",
    "        out.write(word)\n",
    "        out.write(\"\\n\")\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3oxjX4evHje0"
   },
   "source": [
    "#### 切分训练，验证，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-8TqvFkHqbB"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!head -300000 question.file > data/train.input\n",
    "!head -300000 answer.file > data/train.output\n",
    "!head -380000 question.file | tail -80000 > data/val.input\n",
    "!head -380000 answer.file | tail -80000 > data/val.output\n",
    "!tail -75000 question.file > data/test.input\n",
    "!tail -75000 answer.file > data/test.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQ2ooLx9DuB3"
   },
   "source": [
    "**构建词表**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNy2gLOADuB4"
   },
   "outputs": [],
   "source": [
    "in_file = \"question.file\"\n",
    "out_file = \"./data/vocab.input\"\n",
    "get_vocab(in_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1LlbosEDuB5"
   },
   "outputs": [],
   "source": [
    "in_file = \"answer.file\"\n",
    "out_file = \"./data/vocab.output\"\n",
    "get_vocab(in_file, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALzBFu4LDuB8"
   },
   "source": [
    "**新建文件夹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORzsXuC3DuB9"
   },
   "outputs": [],
   "source": [
    "!mkdir /tmp/nmt_attention_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgdXPZpfDuB-"
   },
   "source": [
    "**训练摘要生成模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4232
    },
    "colab_type": "code",
    "id": "uSQnnDoeDuB_",
    "outputId": "d30685a3-e3ec-4ca2-c21f-a766f0262e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job id 0\n",
      "# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 9021483762835584285), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 656894764299535912)]\n",
      "# Loading hparams from /tmp/nmt_attention_model/hparams\n",
      "# Vocab file ./data/vocab.input exists\n",
      "# Vocab file ./data/vocab.output exists\n",
      "  saving hparams to /tmp/nmt_attention_model/hparams\n",
      "  saving hparams to /tmp/nmt_attention_model/best_bleu/hparams\n",
      "  attention=scaled_luong\n",
      "  attention_architecture=standard\n",
      "  avg_ckpts=False\n",
      "  batch_size=128\n",
      "  beam_width=0\n",
      "  best_bleu=0\n",
      "  best_bleu_dir=/tmp/nmt_attention_model/best_bleu\n",
      "  check_special_token=True\n",
      "  colocate_gradients_with_ops=True\n",
      "  decay_scheme=\n",
      "  dev_prefix=./data/val\n",
      "  dropout=0.2\n",
      "  embed_prefix=None\n",
      "  encoder_type=uni\n",
      "  eos=</s>\n",
      "  epoch_step=0\n",
      "  forget_bias=1.0\n",
      "  infer_batch_size=32\n",
      "  infer_mode=greedy\n",
      "  init_op=uniform\n",
      "  init_weight=0.1\n",
      "  language_model=False\n",
      "  learning_rate=1.0\n",
      "  length_penalty_weight=0.0\n",
      "  log_device_placement=False\n",
      "  max_gradient_norm=5.0\n",
      "  max_train=0\n",
      "  metrics=['bleu']\n",
      "  num_buckets=5\n",
      "  num_dec_emb_partitions=0\n",
      "  num_decoder_layers=2\n",
      "  num_decoder_residual_layers=0\n",
      "  num_embeddings_partitions=0\n",
      "  num_enc_emb_partitions=0\n",
      "  num_encoder_layers=2\n",
      "  num_encoder_residual_layers=0\n",
      "  num_gpus=1\n",
      "  num_inter_threads=0\n",
      "  num_intra_threads=0\n",
      "  num_keep_ckpts=5\n",
      "  num_sampled_softmax=0\n",
      "  num_train_steps=12000\n",
      "  num_translations_per_input=1\n",
      "  num_units=128\n",
      "  optimizer=sgd\n",
      "  out_dir=/tmp/nmt_attention_model\n",
      "  output_attention=True\n",
      "  override_loaded_hparams=False\n",
      "  pass_hidden_state=True\n",
      "  random_seed=None\n",
      "  residual=False\n",
      "  sampling_temperature=0.0\n",
      "  share_vocab=False\n",
      "  sos=<s>\n",
      "  src=input\n",
      "  src_embed_file=\n",
      "  src_max_len=50\n",
      "  src_max_len_infer=None\n",
      "  src_vocab_file=./data/vocab.input\n",
      "  src_vocab_size=56491\n",
      "  steps_per_external_eval=None\n",
      "  steps_per_stats=100\n",
      "  subword_option=\n",
      "  test_prefix=./data/test\n",
      "  tgt=output\n",
      "  tgt_embed_file=\n",
      "  tgt_max_len=50\n",
      "  tgt_max_len_infer=None\n",
      "  tgt_vocab_file=./data/vocab.output\n",
      "  tgt_vocab_size=50041\n",
      "  time_major=True\n",
      "  train_prefix=./data/train\n",
      "  unit_type=lstm\n",
      "  use_char_encode=False\n",
      "  vocab_prefix=./data/vocab\n",
      "  warmup_scheme=t2t\n",
      "  warmup_steps=0\n",
      "WARNING:tensorflow:From /content/nmt/nmt/utils/iterator_utils.py:235: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.group_by_window(...)`.\n",
      "# Creating train graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=1, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0\n",
      "# Creating eval graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0\n",
      "# Creating infer graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), \n",
      "# log_file=/tmp/nmt_attention_model/log_1548576134\n",
      "  created train model with fresh parameters, time 0.34s\n",
      "  created infer model with fresh parameters, time 0.26s\n",
      "  # 8630\n",
      "    src: 老子 不 搞\n",
      "    ref: 你 有 啥 不 高兴 的 事 ， 说 出来 让 大家 开心 一下\n",
      "    nmt: 熄灯 幹 卡则 幹 陈虹 陈虹\n",
      "  created eval model with fresh parameters, time 0.29s\n",
      "2019-01-27 08:03:58.745213: W tensorflow/core/framework/allocator.cc:122] Allocation of 4919230464 exceeds 10% of system memory.\n",
      "tcmalloc: large alloc 4919230464 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "2019-01-27 08:05:12.886265: W tensorflow/core/framework/allocator.cc:122] Allocation of 999218688 exceeds 10% of system memory.\n",
      "2019-01-27 08:06:03.966427: W tensorflow/core/framework/allocator.cc:122] Allocation of 3996874752 exceeds 10% of system memory.\n",
      "tcmalloc: large alloc 3996876800 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "2019-01-27 08:07:30.739635: W tensorflow/core/framework/allocator.cc:122] Allocation of 1255428608 exceeds 10% of system memory.\n",
      "2019-01-27 08:07:43.000768: W tensorflow/core/framework/allocator.cc:122] Allocation of 3791906816 exceeds 10% of system memory.\n",
      "tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 3971260416 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 4201848832 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 3996876800 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "  eval dev: perplexity 50033.89, time 906s, Sun Jan 27 08:17:23 2019.\n",
      "tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "  eval test: perplexity 50037.16, time 815s, Sun Jan 27 08:30:59 2019.\n",
      "2019-01-27 08:30:59.371143: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 08:30:59.371143: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 08:30:59.371386: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  created infer model with fresh parameters, time 0.22s\n",
      "# Start step 0, lr 1, Sun Jan 27 08:30:59 2019\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 1 step-time 2.60s wps 0.56K ppl 17827.47 gN 21.16 bleu 0.00, Sun Jan 27 08:35:19 2019\n",
      "  step 200 lr 1 step-time 2.21s wps 0.65K ppl 976.06 gN 5.97 bleu 0.00, Sun Jan 27 08:39:00 2019\n",
      "  step 300 lr 1 step-time 2.17s wps 0.67K ppl 552.16 gN 3.62 bleu 0.00, Sun Jan 27 08:42:38 2019\n",
      "  step 400 lr 1 step-time 2.38s wps 0.66K ppl 590.66 gN 4.01 bleu 0.00, Sun Jan 27 08:46:36 2019\n",
      "  step 500 lr 1 step-time 2.30s wps 0.66K ppl 433.11 gN 3.06 bleu 0.00, Sun Jan 27 08:50:26 2019\n",
      "  step 600 lr 1 step-time 2.30s wps 0.66K ppl 347.75 gN 2.77 bleu 0.00, Sun Jan 27 08:54:15 2019\n",
      "  step 700 lr 1 step-time 2.32s wps 0.66K ppl 314.29 gN 2.56 bleu 0.00, Sun Jan 27 08:58:07 2019\n",
      "  step 800 lr 1 step-time 2.21s wps 0.66K ppl 246.35 gN 2.13 bleu 0.00, Sun Jan 27 09:01:48 2019\n",
      "  step 900 lr 1 step-time 2.21s wps 0.65K ppl 224.77 gN 2.48 bleu 0.00, Sun Jan 27 09:05:29 2019\n",
      "  step 1000 lr 1 step-time 2.33s wps 0.66K ppl 223.08 gN 2.10 bleu 0.00, Sun Jan 27 09:09:22 2019\n",
      "# Save eval, global step 1000\n",
      "2019-01-27 09:09:23.307011: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 09:09:23.307011: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 09:09:23.307277: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.11s\n",
      "  # 41510\n",
      "    src: 说个 傅里叶 变换\n",
      "    ref: =   =\n",
      "    nmt: <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "2019-01-27 09:09:23.459672: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 09:09:23.459672: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 09:09:23.460115: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.12s\n",
      "tcmalloc: large alloc 4919230464 bytes == 0x5241a000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "  eval dev: perplexity 328.47, time 897s, Sun Jan 27 09:24:21 2019.\n",
      "  eval test: perplexity 436.29, time 812s, Sun Jan 27 09:37:54 2019.\n",
      "  step 1100 lr 1 step-time 2.27s wps 0.65K ppl 208.46 gN 2.07 bleu 0.00, Sun Jan 27 09:41:41 2019\n",
      "  step 1200 lr 1 step-time 2.26s wps 0.65K ppl 187.18 gN 2.13 bleu 0.00, Sun Jan 27 09:45:27 2019\n",
      "  step 1300 lr 1 step-time 2.36s wps 0.66K ppl 208.60 gN 2.36 bleu 0.00, Sun Jan 27 09:49:23 2019\n",
      "  step 1400 lr 1 step-time 2.31s wps 0.65K ppl 179.46 gN 2.10 bleu 0.00, Sun Jan 27 09:53:14 2019\n",
      "  step 1500 lr 1 step-time 2.24s wps 0.65K ppl 162.42 gN 2.01 bleu 0.00, Sun Jan 27 09:56:58 2019\n",
      "  step 1600 lr 1 step-time 2.45s wps 0.66K ppl 203.35 gN 2.22 bleu 0.00, Sun Jan 27 10:01:03 2019\n",
      "  step 1700 lr 1 step-time 2.32s wps 0.65K ppl 157.10 gN 1.97 bleu 0.00, Sun Jan 27 10:04:55 2019\n",
      "  step 1800 lr 1 step-time 2.31s wps 0.66K ppl 165.35 gN 2.15 bleu 0.00, Sun Jan 27 10:08:46 2019\n",
      "  step 1900 lr 1 step-time 2.31s wps 0.66K ppl 151.73 gN 2.09 bleu 0.00, Sun Jan 27 10:12:37 2019\n",
      "  step 2000 lr 1 step-time 2.26s wps 0.66K ppl 133.99 gN 1.79 bleu 0.00, Sun Jan 27 10:16:23 2019\n",
      "# Save eval, global step 2000\n",
      "2019-01-27 10:16:23.894014: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 10:16:23.894014: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 10:16:23.894265: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.10s\n",
      "  # 51767\n",
      "    src: 讲个 黄色笑话 呗\n",
      "    ref: 二\n",
      "    nmt: <unk> <unk> <unk>\n",
      "2019-01-27 10:16:24.020441: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 10:16:24.020441: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 10:16:24.020711: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.11s\n",
      "  eval dev: perplexity 133.70, time 901s, Sun Jan 27 10:31:25 2019.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m nmt.nmt \\\n",
    "    --attention=scaled_luong \\\n",
    "    --src=input --tgt=output \\\n",
    "    --vocab_prefix=./data/vocab  \\\n",
    "    --train_prefix=./data/train \\\n",
    "    --dev_prefix=./data/val  \\\n",
    "    --test_prefix=./data/test \\\n",
    "    --out_dir=/tmp/nmt_attention_model \\\n",
    "    --num_train_steps=12000 \\\n",
    "    --steps_per_stats=1 \\\n",
    "    --num_layers=2 \\\n",
    "    --num_units=128 \\\n",
    "    --dropout=0.2 \\\n",
    "    --metrics=bleu"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3.seq2seq_application_step_by_step.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Transformer的聊天机器人构建\n",
    "代码大家可以参考[Transformer-in-generating-dialogue](https://github.com/EternalFeather/Transformer-in-generating-dialogue)的实现，基于上述代码的小黄鸡对话语料构建聊天机器人版本可以参考[transformer-chatbot](https://github.com/dengqiqi123/transformer-chatbot.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformer-chatbot'...\n",
      "remote: Enumerating objects: 46, done.\u001b[K\n",
      "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
      "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
      "Unpacking objects:  21% (10/46)   \r"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/dengqiqi123/transformer-chatbot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  getData.py  main.py  model  model.py  modules.py\ttrain.py  utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls transformer-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd transformer-chatbot/\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load transformer-chatbot/utils.py\n",
    "import codecs\n",
    "import csv\n",
    "import array\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import jieba\n",
    "import logging\n",
    "import os\n",
    "def create_model_and_embedding(session,Model_class,path,config,is_train):\n",
    "    model = Model_class(config,is_train)\n",
    "    ckpt = tf.train.get_checkpoint_state(path) \n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model \n",
    "def save_model(sess, model, path,logger):\n",
    "    checkpoint_path = os.path.join(path, \"chatbot.ckpt\")\n",
    "    model.saver.save(sess, checkpoint_path)\n",
    "    logger.info(\"model saved\")\n",
    "def load_sor_vocab():\n",
    "    vocab = [line.split()[0] for line in codecs.open('data/vocab.tsv', 'r', 'utf-8').read().splitlines()]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return word2idx, idx2word    \n",
    "def load_mub_vocab():   \n",
    "    vocab = [line.split()[0] for line in codecs.open('data/vocab.answer.tsv', 'r', 'utf-8').read().splitlines()]\n",
    "    #word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    #idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    #return word2idx, idx2word    \n",
    "def load_sentences(sor_path,mub_path):\n",
    "    de_sents = [line.strip().replace('\\r','') for line in codecs.open(sor_path, 'r', 'utf-8').read().split(\"\\n\")]\n",
    "    en_sents = [line.strip().replace('\\r','') for line in codecs.open(mub_path, 'r', 'utf-8').read().split(\"\\n\")]\n",
    "    de_sents = [' '.join([i for i in line.strip()])  for line in de_sents]\n",
    "    en_sents = [' '.join([i for i in line.strip()])  for line in en_sents]\n",
    "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
    "    return X, Y \n",
    "def create_data(source_sents, target_sents):\n",
    "    word2id,id2word = load_sor_vocab()\n",
    "    #mub2id,id2mud = load_mub_vocab()\n",
    "    x_list, y_list, Sources, Targets = [], [], [], []\n",
    "    for source_sent, target_sent in zip(source_sents, target_sents):\n",
    "        x = [word2id.get(word, 1) for word in (source_sent).split()] # 1: OOV, </S>: End of Text\n",
    "        y = [word2id.get(word, 1) for word in (target_sent+\" </S>\").split()] \n",
    "        if max(len(x), len(y)) <= 20:\n",
    "            x_list.append(np.array(x))\n",
    "            y_list.append(np.array(y))\n",
    "            Sources.append(source_sent)\n",
    "            Targets.append(target_sent)\n",
    "    return x_list, y_list, Sources, Targets \n",
    "#实例化日志类\n",
    "def get_logger(log_file):\n",
    "    logger = logging.getLogger(log_file)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    ch.setFormatter(formatter)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    return logger\n",
    "def input_from_line(line, char_to_id):\n",
    "    inputs = list()\n",
    "    #把空格替换为$\n",
    "    line = line.replace(\" \", \"\")    \n",
    "    #查字典，把输入字符中能查到字典的字符转换为ID值，查不到的字标记为<UNK>\n",
    "    ids = [char_to_id[char] if char in char_to_id else char_to_id[\"<UNK>\"] for char in line] \n",
    "    #+[char_to_id['</S>']]\n",
    "    inputs.append([ids])\n",
    "    inputs.append([line])\n",
    "    return inputs\n",
    "class BatchManager(object):\n",
    "    def __init__(self, sor_data,mub_data,batch_size):\n",
    "        self.batch_data = self.sort_and_pad(sor_data,mub_data,batch_size)\n",
    "        self.len_data = len(self.batch_data)\n",
    "    def sort_and_pad(self,sor_data,mub_data, batch_size):\n",
    "        alldata = []\n",
    "        for ask,answer in zip(sor_data, mub_data):\n",
    "            sentence = []\n",
    "            sentence.append(ask)\n",
    "            sentence.append(answer)\n",
    "            alldata.append(sentence)\n",
    "        num_batch = int(math.ceil(len(alldata) /batch_size))\n",
    "        \n",
    "        #sorted_data = sorted(sor_data, key=lambda x: len(x[0]))\n",
    "        #sorted_data = sor_data\n",
    "               \n",
    "        random.shuffle(alldata)\n",
    "        batch_data = []\n",
    "        for i in range(num_batch):\n",
    "            batch_data.append(self.pad_data(alldata[i*int(batch_size) : (i+1)*int(batch_size)]))\n",
    "        return batch_data\n",
    "    @staticmethod\n",
    "    def pad_data(data):\n",
    "        ask,answer = [],[]\n",
    "        max_sor = max([len(sentence[0]) for sentence in data])\n",
    "        max_mub = max([len(sentence[1]) for sentence in data])\n",
    "        for line in data:\n",
    "            qpadding = [0] * (max_sor- len(line[0]))\n",
    "            ask.append(list(line[0])+qpadding)\n",
    "            apadding = [0] * (max_mub - len(line[1]))\n",
    "            answer.append(list(line[1])+apadding)            \n",
    "        return [ask,answer]\n",
    "    def iter_batch(self, shuffle=False):\n",
    "        if shuffle:\n",
    "            random.shuffle(self.batch_data)\n",
    "        for idx in range(self.len_data):\n",
    "            yield self.batch_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load transformer-chatbot/getData.py\n",
    "#enconding=utf-8\n",
    "import os,sys,csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import tensorflow as tf\n",
    "from modules import *\n",
    "\n",
    "def full_to_half(s):\n",
    "    \"\"\"\n",
    "    将全角字符转换为半角字符 \n",
    "    \"\"\"\n",
    "    n = []\n",
    "    for char in s:\n",
    "        num = ord(char)\n",
    "        if num == 0x3000:\n",
    "            num = 32\n",
    "        elif 0xFF01 <= num <= 0xFF5E:\n",
    "            num -= 0xfee0\n",
    "        char = chr(num)\n",
    "        n.append(char)\n",
    "    return ''.join(n)\n",
    "\n",
    "def replace_html(s):\n",
    "    s = s.replace('&quot;','\"')\n",
    "    s = s.replace('&amp;','&')\n",
    "    s = s.replace('&lt;','<')\n",
    "    s = s.replace('&gt;','>')\n",
    "    s = s.replace('&nbsp;',' ')\n",
    "    s = s.replace(\"&ldquo;\", \"\")\n",
    "    s = s.replace(\"&rdquo;\", \"\")\n",
    "    s = s.replace(\"&mdash;\",\"\")\n",
    "    s = s.replace(\"\\xa0\", \" \")\n",
    "    return(s)\n",
    "def setdata(line):\n",
    "    line = line.replace('。','')\n",
    "    line = line.replace('？','')\n",
    "    line = line.replace('！','')\n",
    "    line = line.replace('，','')\n",
    "    line = line.replace('.','')\n",
    "    line = line.replace(',','')\n",
    "    line = line.replace('?','')\n",
    "    line = line.replace('!','')\n",
    "    line = line.replace('“','')\n",
    "    line = line.replace('”','')\n",
    "    return line\n",
    "'''\n",
    "y = tf.constant([[4,2,3,4,5,6,7,8,9]])\n",
    "enc = embedding(y, \n",
    "                         vocab_size=20, \n",
    "                                  num_units=8, \n",
    "                                  scale=True,\n",
    "                                  scope=\"enc_embed\")\n",
    "\n",
    "key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(enc), axis=-1)), -1)\n",
    "with tf.Session() as sess:\n",
    "    initall = tf.global_variables_initializer()\n",
    "    sess.run(initall)\n",
    "    print(sess.run(key_masks))\n",
    "'''\n",
    "vocab = {line.split()[0]:int(line.split()[1]) for line in codecs.open('data/vocab.tsv', 'r', 'utf-8').read().splitlines()}\n",
    "fp = codecs.open('data/train.answer.tsv','r',encoding='utf-8-sig').read().split('\\n')\n",
    "#vocab = {}\n",
    "for w in fp:\n",
    "    for i in w.strip():\n",
    "        if i in vocab.keys():\n",
    "            vocab[i] += 1\n",
    "        else:\n",
    "            vocab[i] = 1\n",
    "\n",
    "with open('data/vocab.tsv','w',encoding='utf-8') as fa:\n",
    "    for k,v in vocab.items():\n",
    "        strs = k+' '+str(v)\n",
    "        fa.write(strs+'\\n')\n",
    "fa.close()\n",
    "'''\n",
    "fp = codecs.open('data/xiaohuangji50w_nofenci.conv','r',encoding='utf-8')\n",
    "i = 1\n",
    "asks = []\n",
    "answers = []\n",
    "sentence = []\n",
    "for k,w in enumerate(fp):\n",
    "    w = w.strip()\n",
    "    if k > 0:\n",
    "        if \"M\" not in w and w != 'E':\n",
    "            continue        \n",
    "        if i%3 == 0:\n",
    "            sentence[1] = sentence[1].replace(' ','')\n",
    "            sentence[2] = sentence[2].replace(' ','')\n",
    "            if sentence[1][1:] != '' and sentence[2][1:] != '':\n",
    "                asks.append(sentence[1][1:])\n",
    "                answers.append(sentence[2][1:])\n",
    "            sentence = []\n",
    "            i = 1\n",
    "            sentence.append(w)\n",
    "        else:\n",
    "            i += 1\n",
    "            sentence.append(w)\n",
    "    else:\n",
    "        sentence.append(w)\n",
    "asks = list(filter(None,asks))\n",
    "answers = list(filter(None,answers))\n",
    "'''\n",
    "fp = codecs.open('data/123.txt','r',encoding='utf-8-sig')\n",
    "i = 1\n",
    "asks = []\n",
    "answers = []\n",
    "for k,w in enumerate(fp):\n",
    "    w = w.strip()\n",
    "    w = full_to_half(w)\n",
    "    w = replace_html(w)    \n",
    "    w = setdata(w)\n",
    "    if k%2 == 0:\n",
    "        asks.append(w)\n",
    "    else:\n",
    "        answers.append(w)\n",
    "with open('data/train.ask.tsv','w',encoding='utf-8') as fa:\n",
    "    for w in asks:\n",
    "        fa.write(w+'\\n')\n",
    "with open('data/train.answer.tsv','w',encoding='utf-8') as fs:\n",
    "    for w in answers:\n",
    "        fs.write(w+'\\n')\n",
    "fa.close()\n",
    "fs.close()\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load transformer-chatbot/model.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils import load_sor_vocab,load_mub_vocab\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "from modules import *\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, config,is_train=True):\n",
    "        self.is_train =  is_train\n",
    "        self.config = config\n",
    "        self.lr = config[\"learning_rate\"]\n",
    "        self.maxlen = config['sequence_length']\n",
    "        self.dropout_rate = config['dropout_rate']\n",
    "        self.hidden_units = config['hidden_units']\n",
    "        self.num_blocks = config['num_blocks']\n",
    "        self.num_heads = config['num_heads']        \n",
    "        \n",
    "        self.global_step = tf.Variable(0,trainable=False)  \n",
    "        #定义编码输入input\n",
    "        self.sor_inputs = tf.placeholder(dtype=tf.int32,shape=[None,None],name='sorinput')\n",
    "        #定义编码输入output\n",
    "        self.out_inputs = tf.placeholder(dtype=tf.int32,shape=[None,None],name='outinput')\n",
    "        self.decode_input = tf.concat((tf.ones_like(self.out_inputs[:, :1])*2, self.out_inputs[:, :-1]), -1)\n",
    "        word2id,id2word = load_sor_vocab()\n",
    "        # Encoder\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            self.enc = embedding(self.sor_inputs, len(word2id), self.hidden_units,scale=True,scope=\"enc_embed\")\n",
    "            key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(self.enc), axis=-1)), -1)\n",
    "            # Positional Encoding\n",
    "            if False:\n",
    "                self.enc += positional_encoding(self.sor_inputs,num_units=self.hidden_units,zero_pad=False,scale=False,scope=\"enc_pe\")\n",
    "            else:\n",
    "                self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.sor_inputs)[1]), 0), [tf.shape(self.sor_inputs)[0], 1]),vocab_size=self.maxlen, \n",
    "                                      num_units=self.hidden_units,zero_pad=False,scale=False,scope=\"enc_pe\")\n",
    "\n",
    "            self.enc *= key_masks\n",
    "            # Dropout\n",
    "            self.enc = tf.layers.dropout(self.enc,rate=self.dropout_rate,training=tf.convert_to_tensor(self.is_train))   \n",
    "            # Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    # Multihead Attention\n",
    "                    self.enc = multihead_attention(queries=self.enc,keys=self.enc,num_units=self.hidden_units,num_heads=self.num_heads,dropout_rate=self.dropout_rate,is_training=self.is_train,\n",
    "                                                                causality=False)\n",
    "                    # Feed Forward\n",
    "                    self.enc = feedforward(self.enc, num_units=[4*self.hidden_units, self.hidden_units])  \n",
    "        #Decode\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # Embedding\n",
    "            self.dec = embedding(self.decode_input,vocab_size=len(word2id),num_units=self.hidden_units,scale=True,scope=\"dec_embed\") \n",
    "            key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(self.dec), axis=-1)), -1)\n",
    "            # Positional Encoding\n",
    "            if False:\n",
    "                self.dec += positional_encoding(self.decode_input,vocab_size=self.maxlen,num_units=self.hidden_units,zero_pad=False,scale=False,scope=\"dec_pe\")\n",
    "            else:\n",
    "                self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decode_input)[1]), 0), [tf.shape(self.decode_input)[0], 1]),vocab_size=self.maxlen,num_units=self.hidden_units, \n",
    "                                              zero_pad=False, \n",
    "                                              scale=False,\n",
    "                                              scope=\"dec_pe\")\n",
    "            self.dec *= key_masks \n",
    "            # Dropout\n",
    "            self.dec = tf.layers.dropout(self.dec,rate=self.dropout_rate,training=tf.convert_to_tensor(self.is_train)) \n",
    "            # Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    # Multihead Attention ( self-attention)\n",
    "                    self.dec = multihead_attention(queries=self.dec,keys=self.dec,num_units=self.hidden_units,num_heads=self.num_heads, dropout_rate=self.dropout_rate,is_training=self.is_train,\n",
    "                                                                causality=True, \n",
    "                                                                scope=\"self_attention\")\n",
    "                    # Multihead Attention ( vanilla attention)\n",
    "                    self.dec = multihead_attention(queries=self.dec,keys=self.enc,num_units=self.hidden_units,num_heads=self.num_heads,dropout_rate=self.dropout_rate,is_training=self.is_train, \n",
    "                                                                causality=False,\n",
    "                                                                scope=\"vanilla_attention\")\n",
    "                    # Feed Forward\n",
    "                    self.dec = feedforward(self.dec, num_units=[4*self.hidden_units, self.hidden_units]) \n",
    "        # Final linear projection\n",
    "        self.logits = tf.layers.dense(self.dec, len(word2id))\n",
    "        self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n",
    "        self.istarget = tf.to_float(tf.not_equal(self.out_inputs, 0))\n",
    "        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.out_inputs))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
    "        #tf.summary.scalar('acc', self.acc)   \n",
    "        # Loss\n",
    "        self.y_smoothed = label_smoothing(tf.one_hot(self.out_inputs, depth=len(word2id)))\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
    "        self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "       \n",
    "        # 定义优化器\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr)#, beta1=0.9, beta2=0.98, epsilon=1e-8\n",
    "            grads_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            capped_grads_vars = [[tf.clip_by_value(g,-5,5),v] for g,v in grads_vars]        \n",
    "            self.train_op = self.optimizer.apply_gradients(capped_grads_vars,self.global_step)\n",
    "        self.saver = tf.train.Saver(tf.global_variables(),max_to_keep=1)\n",
    "    def create_feed_dict(self,is_train,batch):\n",
    "        if is_train:\n",
    "            ask,answer = batch\n",
    "            feed_dict = {\n",
    "                self.sor_inputs: np.asarray(ask),\n",
    "                self.out_inputs: np.asarray(answer)\n",
    "            }\n",
    "        else:\n",
    "            ask,_ = batch\n",
    "            feed_dict = {\n",
    "            #self.sor_inputs: np.asarray(ask),\n",
    "            #self.out_inputs:np.zeros((1, len(ask[0])), np.int32)\n",
    "            }            \n",
    "        return feed_dict        \n",
    "    def run_step(self,sess,is_train,batch):\n",
    "        feed_dict = self.create_feed_dict(is_train,batch)\n",
    "        if is_train:\n",
    "            global_step,y_smoothed,loss,logits,preds,_ = sess.run([self.global_step,self.y_smoothed,self.mean_loss,self.logits,self.preds,self.train_op],feed_dict)                 \n",
    "            return global_step, loss\n",
    "        else:\n",
    "            ask,_ = batch\n",
    "            preds = np.ones((1,20), np.int32)\n",
    "            #preds[:,0] = 2\n",
    "            #preds[:,19] = 3\n",
    "            for i in range(20):\n",
    "                _preds = sess.run(self.preds, {self.sor_inputs: np.asarray(ask), self.out_inputs:preds})\n",
    "                preds[:,i] = _preds[:,i]                \n",
    "            #preds = sess.run([self.preds], feed_dict)\n",
    "            return preds\n",
    "    def evaluate_line(self, sess, inputs):\n",
    "        probs = self.run_step(sess, False, inputs)\n",
    "        return probs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load transformer-chatbot/modules.py\n",
    "#/usr/bin/python2\n",
    "'''\n",
    "June 2017 by kyubyong park. \n",
    "kbpark.linguist@gmail.com.\n",
    "https://www.github.com/kyubyong/transformer\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(inputs, \n",
    "              epsilon = 1e-8,\n",
    "              scope=\"ln\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "def embedding(inputs, \n",
    "              vocab_size, \n",
    "              num_units, \n",
    "              zero_pad=True, \n",
    "              scale=True,\n",
    "              scope=\"embedding\", \n",
    "              reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "        \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]    \n",
    "    ```    \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        \n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5) \n",
    "            \n",
    "    return outputs\n",
    "    \n",
    "\n",
    "def positional_encoding(inputs,\n",
    "                        num_units,\n",
    "                        zero_pad=True,\n",
    "                        scale=True,\n",
    "                        scope=\"positional_encoding\",\n",
    "                        reuse=None):\n",
    "    '''Sinusoidal Positional_Encoding.\n",
    "\n",
    "    Args:\n",
    "      inputs: A 2d Tensor with shape of (N, T).\n",
    "      num_units: Output dimensionality\n",
    "      zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\n",
    "      scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "        A 'Tensor' with one more rank than inputs's, with the dimensionality should be 'num_units'\n",
    "    '''\n",
    "\n",
    "    N, T = inputs.get_shape().as_list()\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n",
    "\n",
    "        # First part of the PE function: sin and cos argument\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)]\n",
    "            for pos in range(T)])\n",
    "\n",
    "        # Second part, apply the cosine to even columns and sin to odds.\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        # Convert to a tensor\n",
    "        lookup_table = tf.convert_to_tensor(position_enc)\n",
    "\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n",
    "\n",
    "        if scale:\n",
    "            outputs = outputs * num_units**0.5\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def multihead_attention(queries, \n",
    "                        keys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list()[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "        \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1)) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "        \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "   \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "         \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1)) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs\n",
    "\n",
    "def feedforward(inputs, \n",
    "                num_units=[2048, 512],\n",
    "                scope=\"multihead_attention\", \n",
    "                reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```    \n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load transformer-chatbot/train.py\n",
    "#/usr/bin/python2\n",
    "'''\n",
    "June 2017 by kyubyong park. \n",
    "kbpark.linguist@gmail.com.\n",
    "https://www.github.com/kyubyong/transformer\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "from hyperparams import Hyperparams as hp\n",
    "from data_load import get_batch_data, load_de_vocab, load_en_vocab\n",
    "from modules import *\n",
    "import os, codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Graph():\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if is_training:\n",
    "                self.x, self.y, self.num_batch = get_batch_data() # (N, T)\n",
    "            else: # inference\n",
    "                self.x = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "                self.y = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "\n",
    "            # define decoder inputs\n",
    "            self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 2:<S>\n",
    "\n",
    "            # Load vocabulary    \n",
    "            de2idx, idx2de = load_de_vocab()\n",
    "            en2idx, idx2en = load_en_vocab()\n",
    "            \n",
    "            # Encoder\n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                ## Embedding\n",
    "                self.enc = embedding(self.x, \n",
    "                                      vocab_size=len(de2idx), \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      scale=True,\n",
    "                                      scope=\"enc_embed\")\n",
    "\n",
    "                key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(self.enc), axis=-1)), -1)\n",
    "\n",
    "                ## Positional Encoding\n",
    "                if hp.sinusoid:\n",
    "                    self.enc += positional_encoding(self.x,\n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"enc_pe\")\n",
    "                else:\n",
    "                    self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"enc_pe\")\n",
    "\n",
    "                self.enc *= key_masks\n",
    "                 \n",
    "                ## Dropout\n",
    "                self.enc = tf.layers.dropout(self.enc, \n",
    "                                            rate=hp.dropout_rate, \n",
    "                                            training=tf.convert_to_tensor(is_training))\n",
    "                \n",
    "                ## Blocks\n",
    "                for i in range(hp.num_blocks):\n",
    "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                        ### Multihead Attention\n",
    "                        self.enc = multihead_attention(queries=self.enc, \n",
    "                                                        keys=self.enc, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads, \n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training,\n",
    "                                                        causality=False)\n",
    "                        \n",
    "                        ### Feed Forward\n",
    "                        self.enc = feedforward(self.enc, num_units=[4*hp.hidden_units, hp.hidden_units])\n",
    "            \n",
    "            # Decoder\n",
    "            with tf.variable_scope(\"decoder\"):\n",
    "                ## Embedding\n",
    "                self.dec = embedding(self.decoder_inputs, \n",
    "                                      vocab_size=len(en2idx), \n",
    "                                      num_units=hp.hidden_units,\n",
    "                                      scale=True, \n",
    "                                      scope=\"dec_embed\")\n",
    "\n",
    "                key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(self.dec), axis=-1)), -1)\n",
    "\n",
    "                ## Positional Encoding\n",
    "                if hp.sinusoid:\n",
    "                    self.dec += positional_encoding(self.decoder_inputs,\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"dec_pe\")\n",
    "                else:\n",
    "                    self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0), [tf.shape(self.decoder_inputs)[0], 1]),\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"dec_pe\")\n",
    "                self.dec *= key_masks\n",
    "                \n",
    "                ## Dropout\n",
    "                self.dec = tf.layers.dropout(self.dec, \n",
    "                                            rate=hp.dropout_rate, \n",
    "                                            training=tf.convert_to_tensor(is_training))\n",
    "                \n",
    "                ## Blocks\n",
    "                for i in range(hp.num_blocks):\n",
    "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                        ## Multihead Attention ( self-attention)\n",
    "                        self.dec = multihead_attention(queries=self.dec, \n",
    "                                                        keys=self.dec, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads, \n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention\")\n",
    "                        \n",
    "                        ## Multihead Attention ( vanilla attention)\n",
    "                        self.dec = multihead_attention(queries=self.dec, \n",
    "                                                        keys=self.enc, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads,\n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training, \n",
    "                                                        causality=False,\n",
    "                                                        scope=\"vanilla_attention\")\n",
    "                        \n",
    "                        ## Feed Forward\n",
    "                        self.dec = feedforward(self.dec, num_units=[4*hp.hidden_units, hp.hidden_units])\n",
    "                \n",
    "            # Final linear projection\n",
    "            self.logits = tf.layers.dense(self.dec, len(en2idx))\n",
    "            self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n",
    "            self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "            self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
    "            tf.summary.scalar('acc', self.acc)\n",
    "                \n",
    "            if is_training:  \n",
    "                # Loss\n",
    "                self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=len(en2idx)))\n",
    "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
    "                self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "               \n",
    "                # Training Scheme\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "                self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "                # Summary \n",
    "                tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "                self.merged = tf.summary.merge_all()\n",
    "\n",
    "if __name__ == '__main__':                \n",
    "    # Load vocabulary    \n",
    "    de2idx, idx2de = load_de_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "    \n",
    "    # Construct graph\n",
    "    g = Graph(\"train\"); print(\"Graph loaded\")\n",
    "    \n",
    "    # Start session\n",
    "    sv = tf.train.Supervisor(graph=g.graph,logdir=hp.logdir,save_model_secs=0)\n",
    "    with sv.managed_session() as sess:\n",
    "        for epoch in range(1, hp.num_epochs+1): \n",
    "            if sv.should_stop(): break\n",
    "            for step in tqdm(range(g.num_batch), total=g.num_batch, ncols=70, leave=False, unit='b'):\n",
    "                sess.run(g.train_op)\n",
    "                \n",
    "            gs = sess.run(g.global_step)   \n",
    "            sv.saver.save(sess, hp.logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load transformer-chatbot/main.py\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import os, codecs,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import load_sentences,BatchManager,create_model_and_embedding,get_logger,save_model,input_from_line,load_sor_vocab,load_mub_vocab\n",
    "from model import Model\n",
    "from flask import Flask, jsonify, request\n",
    "from collections import OrderedDict\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"block\",6,\"layer_size\")\n",
    "flags.DEFINE_integer(\"sequence_length\",20,\"word vector dim\")\n",
    "flags.DEFINE_integer(\"steps_check\", 10, \"steps per checkpoint\")\n",
    "flags.DEFINE_integer(\"num_of_epoch\", 100000, \"epoch number\")\n",
    "flags.DEFINE_integer(\"batch_size\",64 ,\"word vector dim\")\n",
    "flags.DEFINE_integer('hidden_units',128,'   ')\n",
    "flags.DEFINE_integer('num_blocks',6,'   ')\n",
    "flags.DEFINE_integer('num_heads',8,'   ')\n",
    "flags.DEFINE_float(\"dropout_rate\", 0.0, \"Learning rate\")\n",
    "\n",
    "flags.DEFINE_string(\"model_path\",\"model/\",\"vocab file path\")\n",
    "flags.DEFINE_string(\"train_sor_path\",\"data/train.ask.tsv\",\"train file path\")\n",
    "flags.DEFINE_string(\"train_mub_path\",\"data/train.answer.tsv\",\"train file path\")\n",
    "flags.DEFINE_string(\"logger_path\",\"logger/train.log\",\"vocab file path\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.00001, \"Learning rate\")\n",
    "flags.DEFINE_string(\"optimizer\",    \"adam\",     \"Optimizer for training\")\n",
    "flags.DEFINE_boolean('flag',True,' ')\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "app = Flask(__name__)\n",
    "def config_model():\n",
    "    config = OrderedDict()\n",
    "    config[\"optimizer\"] = FLAGS.optimizer\n",
    "    config[\"layer_size\"] = FLAGS.block\n",
    "    config[\"sequence_length\"] = FLAGS.sequence_length\n",
    "    config[\"batch_size\"] = FLAGS.batch_size\n",
    "    config[\"hidden_units\"] = FLAGS.hidden_units\n",
    "    config[\"num_blocks\"] = FLAGS.num_blocks\n",
    "    config[\"num_heads\"] = FLAGS.num_heads\n",
    "    config[\"dropout_rate\"] = FLAGS.dropout_rate    \n",
    "    \n",
    "    config[\"train_sor_path\"] = FLAGS.train_sor_path\n",
    "    config[\"train_mub_path\"] = FLAGS.train_mub_path\n",
    "    config[\"model_path\"] = FLAGS.model_path\n",
    "    config[\"logger_path\"] = FLAGS.logger_path\n",
    "    config[\"learning_rate\"] = FLAGS.learning_rate\n",
    "    config['flag'] = FLAGS.flag\n",
    "    return config\n",
    "def train():\n",
    "    #加载训练数据并生成可训练数据\n",
    "    train_sor_data,train_mub_data = load_sentences(FLAGS.train_sor_path,FLAGS.train_mub_path)\n",
    "    #将训练数据处理成N批次数据\n",
    "    train_manager = BatchManager(train_sor_data,train_mub_data, FLAGS.batch_size)\n",
    "    #设置gpu参数\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    #加载FLAGS参数\n",
    "    config = config_model()\n",
    "    logger = get_logger(config[\"logger_path\"])\n",
    "    #计算批次数\n",
    "    word2id,id2word = load_sor_vocab() \n",
    "    steps_per_epoch = train_manager.len_data\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        model = create_model_and_embedding(sess, Model, FLAGS.model_path, config,True)\n",
    "        logger.info(\"start training\")\n",
    "        loss = []  \n",
    "        with tf.device('/gpu:0'):\n",
    "            for i in range(FLAGS.num_of_epoch):\n",
    "                for batch in train_manager.iter_batch(shuffle=True):\n",
    "                    step,batch_loss = model.run_step(sess,True,batch)\n",
    "                    loss.append(batch_loss)\n",
    "                    if step%FLAGS.steps_check == 0:\n",
    "                        iteration = step // steps_per_epoch + 1\n",
    "                        logger.info(\"iteration:{} step:{}/{},chatbot loss:{:>9.6f}\".format(iteration, step%steps_per_epoch, steps_per_epoch, np.mean(loss)))\n",
    "                        loss = []\n",
    "                if i%10 == 0:\n",
    "                    save_model(sess, model, FLAGS.model_path,logger) \n",
    "def predict():\n",
    "    word2id,id2word = load_sor_vocab()   \n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    config = config_model() \n",
    "    logger = get_logger(config[\"logger_path\"])  \n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph,config=tf_config)\n",
    "    with graph.as_default():\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        model = create_model_and_embedding(sess, Model, FLAGS.model_path, config,False)\n",
    "        sys.stdout.write('请输入测试句子：')\n",
    "        sys.stdout.flush()\n",
    "        sentences = sys.stdin.readline()\n",
    "        while True:\n",
    "            sentences = sentences.replace('\\n','')        \n",
    "            rs = model.evaluate_line(sess,input_from_line(sentences,word2id))\n",
    "            res = ''.join([id2word[w] for w in rs[0]]).split('</S>')[0].strip()\n",
    "            print(res)\n",
    "            print('请输入测试句子：',end='')\n",
    "            sys.stdout.flush()\n",
    "            sentences = sys.stdin.readline()            \n",
    "        print('ok')\n",
    "def main(_):\n",
    "    predict()\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
